---
title: "STAT2000: Applied Statistics and Research Methods"
output: 
  html_document: 
    css: Labs.css
params:
  inc_solu:
    label: Include solutions
    value: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

if ( ! require('haven'))
  install.packages('haven', repos='https://cloud.r-project.org')
if ( ! require('car'))
  install.packages('car', repos='https://cloud.r-project.org')
if ( ! require('NSM3'))
  install.packages('NSM3', repos='https://cloud.r-project.org')

data <- haven::read_sav('Lab 07 - Parkinsons.sav')
data <- as.data.frame(data)
```

```{block include=(!params$inc_solu)}
<style>
.section.level5 {
  display: none ;
}
</style>
```

# Lab 7 - Week 8

#### Recall Lecture 7:

 - Non-parametric methods
 - Sign test
 - Wilcoxon Signed Rank test
 - Mann-Whitney U test
 - Kruskal-Wallis test

### Lab Objectives

On completion of this lab you will be able to:

 - Identify when it is appropriate to apply non-parametric statistics
 - Apply the appropriate test and interpret the associated results
 - Recognise and summarise the study design used in research articles.

### Overview

In the Week 7 lecture we considered various non-parametric tests that could be used in place of the many parametric tests we have learnt: testing single population, pairs of populations, multiple populations and correlation.

## 7.1 Wilcoxon Signed-Rank Test

A neurologist is interested in the progression of Parkinson’s disease in 15 patients. In particular he wishes to understand if a particular treatment (a form of pallidotomy) provides substantial improvement in their symptoms. Therefore, one month before surgery and three months after surgery he rates the severity of each patient’s disease using a ten-point scale. Scores for Parkinsonian symptoms are given from 1 to 10 with 10 suggesting severe impairment.

The data for each patient is in the table below and [Lab 07 - Parkinsons.sav](Lab 07 - Parkinsons.sav).

 | Patient | Before Surgery | After Surgery |
 |---------|----------------|---------------|
 |    1    |        9       |       3       |
 |    2    |       10       |       4       |
 |    3    |        8       |       6       |
 |    4    |        9       |       4       |
 |    5    |       10       |       9       |
 |    6    |       10       |       5       |
 |    7    |        8       |       5       |
 |    8    |        7       |       7       |
 |    9    |        9       |       2       |
 |   10    |        9       |       9       |
 |   11    |        8       |       6       |
 |   12    |       10       |       7       |
 |   13    |        9       |       6       |
 |   14    |       10       |       6       |
 |   15    |        8       |       5       |


#### 1. Use the Wilcoxon Signed Rank Test to determine whether there has been a statistically significant improvement in symptoms. State the hypotheses, test statistic, p-value and conclusion.

Ⓡ use the function `wilcox.test()`, make sure you specify the `paired=TRUE` option.

Ⓥ Analyses -> T-Tests -> Paired Samples T-Test. As you'll see, the *Wilcoxon signed rank test* is performed the same way you would a paired samples t-test. You just need to check the box for the Wilcoxon signed rank. Make sure you're clear on which results belongs to the parametric Student's t-test, and the non-parametric Wilcoxon signed rank.

##### Solutions

Ⓢ 

$H_0: m_{change} = 0$  or population median change in symptom scores = 0

$H_a: m_{change} \ne 0$

where $m_{change}$  represents the median of change in symptom scores for the population of patients receiving the pallidotomy treatment.

```{r warning=FALSE}
wilcox.test(data$BeforeSurgery, data$AfterSurgery, paired=TRUE)
```

Thus our observed data provide very strong evidence against the null hypothesis (of a population median change in symptoms of zero), p=0.001. This procedure does seem to have some beneficial effect.

#### 2. Use the equivalent parametric test (matched/paired samples t-test) to perform the test.

Ⓡ use the function `t.test()`, make sure you specify the `paired=TRUE` option.

Ⓥ The test statistic and p-value for the Student's t-test should already be visible in the analysis from the previous question.

##### Solutions

Ⓢ 

```{r}
t.test(data$BeforeSurgery, data$AfterSurgery, paired=TRUE)
```

#### 3. Compare your results from 1. and 2.

##### Solutions

Ⓢ 

Results consistent with non-parametric equivalent above. Of the two methods in question, the parametric test is a more powerful test than the non-parametric equivalent for increasing sample size and when the assumptions are met, hence whenever it is possible to use the parametric test it is preferable. In this particular situation the p-values were essentially equally strong and lead to the same conclusion.

## 7.2 Case Study

In Lab 3 we reviewed the following research paper:

de Groot R. H. M., Hornstra G.; Roozendaal N.; Jolles J. (2003) Memory Performance, but not Information Processing Speed, may be Reduced During Early Pregnancy Journal of Clinical and Experimental Neuropsychology, Volume 25, Issue 4, pages 482 -- 488. https://sci-hub.tw/10.1076/jcen.25.4.482.13871

On p484 of this paper, the authors report: "Subject characteristics were examined for statistical differences between the two groups by unpaired t tests. The Mann-Whitney U-test analyzed the differences in education and number of pregnancies." Note that the Mann-Whitney U-test is the non-parametric equivalent to the *Independent samples t-test*.

#### 1. Why was the Mann-Whitney U-test used to examine differences between the control group and the pregnant group for education and number of pregnancies, when other subject characteristics were analysed using unpaired t-tests?

##### Solutions

Ⓢ 

The other subject characteristics Age, Height, Weight could reasonably be taken as continuous variables and therefore have some chance of being analysed reasonably using unpaired t-tests (i.e., 2 sample t-tests). We aren’t given many details about the Education variable except that it was measured on an 8-point scale whilst the number of pregnancies is at best a discrete variable. In the form it is presented in Table 1 of the article, on p485, it is effectively an ordinal variable with 5 values, as we have values for 3 or more pregnancies grouped together into one category.  Certainly these variables are not continuous, let alone normally distributed, and thus do not satisfy the assumptions of a 2 sample t-test. There is no reason why the Mann-Whitney U-test can’t be used however.

#### 2. The raw data for number of pregnancies is given in Table 1 of the article, on p485. Confirm the findings of the authors that there is a statistically significant difference between the two groups with respect to the number of pregnancies.

Consider a Pearson Chi-squared test and Mann-Whitney U-test; why would one be more appropriate in this situation?

Data available in [Lab 07 - Pregnancy.sav](Lab 07 - Pregnancy.sav).

Ⓡ use the function `wilcox.test()`, sometimes Mann-Whitney U is called Wilcoxon signed rank. This data is *not* paired, so don't specify the `paired=TRUE` argument.

Ⓥ Analyses -> T-Tests -> Independent Samples T-Test. You'll find an option for Mann-Whitney U-Test. Make sure you're clear on which results belongs to the parametric Student's t-test, and the non-parametric Mann-Whitney U.

##### Solutions

Ⓢ 

```{r}
data <- haven::read_sav('Lab 07 - Pregnancy.sav')
data <- as.data.frame(data)
wilcox.test(Pregnancies ~ Group, data)
```

Towards the end of p484 of the article, the authors state "The average number of pregnancies was statistically different between the two groups (p = .0014)." Although our basic conclusion is the same, it is unclear why they have reported a p-value so much smaller than what we have obtained; perhaps they undertook a one-tailed test. It is also questionable as to why they have referred to "average number of pregnancies". This would be reasonable if they were in fact conducting a 2 sample t-test but they have clearly stated that they have used a Mann-Whitney U-test for this comparison and expressing the results of this test in terms of averages is poor.

Chi-sq test does not allow for the ordinal nature of the variable representing 'Number of Pregnancies'.

#### 3. Why were the researchers interested in making these comparisons between the two groups?

##### Solutions

Ⓢ 

The researchers are interested in seeing whether the state of being pregnant affects cognitive function.  Therefore the comparisons summarised in Table 1 are to see whether the two comparison groups are fairly similar, except for the fact that one group is pregnant.  If there are other differences between the two groups then such differences could be the reason behind any observed difference in cognitive function rather than the fact that one group is pregnant. For example, the researchers were a bit concerned about the Education variable as it suggested a higher level of education for the Control group compared to the Pregnant group.

Therefore if there is any difference in cognitive function between the groups this might be attributable to differences in education rather than being pregnant or not. Similarly, there is a possibility that as well as the state of being pregnant, the number of pregnancies might affect cognitive function. The researchers have tried to check whether the groups are similar with respect to the number of pregnancies.  Because the two groups were different on number of pregnancies and education, these variables were included as covariates in the ANOVA analysis. In other words, there is some attempt to allow the analysis to compare the two groups while compensating for the effect of education and number of pregnancies if such effects exist. (See Analysis of Covariance concepts in Section 11.2.)

Comment: Viewing the data without doing any of the tests it is fairly obvious that there is a big difference between these groups as the non-pregnant group includes 17 women who have never been pregnant. It could be argued that the control group should only include women who had been pregnant at least once but who were not pregnant at the time to better measure the effect of the state of being pregnant, as opposed to the effect of having had at least one pregnancy.  Probably need another comparison group consisting only of women who had never been pregnant to measure this effect.

## 7.3 Kruskal-Wallis Test

To detect the presence of harmful insects in farm fields, traps consisting of boards covered with a sticky material are used. To determine which colours attract insects best (and therefore make the best traps), researchers placed six boards of each of four colours at random locations in a field of oats and measured the number of cereal leaf beetles trapped.

The data is given in the following table and [Lab 07 - Colour.sav](Lab 07 - Colour.sav).

 | Colour |    |    |    |    |    |    |
 |--------|----|----|----|----|----|----|
 | Lemon  | 45 | 49 | 48 | 46 | 38 | 47 |
 | White  | 21 | 12 | 14 | 17 | 13 | 17 |
 | Green  | 37 | 32 | 15 | 25 | 39 | 41 |
 | Blue   | 16 | 11 | 20 | 21 | 14 |  7 |
	
Is there an effect of colour on the number of insects trapped?

#### 1. Carry out the appropriate ANOVA test. State the hypotheses, test statistic, p-value and conclusion. Remember to test assumptions.

##### Solutions

Ⓢ 

```{r}
data <- haven::read_sav('Lab 07 - Colour.sav')
data <- as.data.frame(data)
data$ColourGrp <- as.factor(data$ColourGrp)
model <- lm(CountTrapped ~ ColourGrp, data)
```

Assumptions (conduct tests for constant error variance and normality of residuals)

```{r}
leveneTest(CountTrapped ~ ColourGrp, data, center='mean')
```

Fail to reject null hypothesis at 5% sig level, hence data are consistent with constant population error variances.

Test normality of residuals

```{r}
resids <- residuals(model)
shapiro.test(resids)
```

p = 0.75 hence fail to reject null and conclude data consistent with residuals following normal distribution.

Since assumptions hold, we can use ANOVA.

ANOVA

$H_0: \alpha_i  = 0$

$H_a: \alpha_i  = 0$ for at least one i

where $\alpha_i$  is the fixed effect of colour i on the population mean number of cereal leaf beetles trapped on traps made from this colour.

OR

$H_0:$ population mean numbers of insects trapped are equal for the four trap colours

$H_a$: at least one coloured trap’s population mean number of insects trapped differs from the others

```{r}
anova(model)
```

The test statistic is F = 30.552 and the corresponding p-value is < 0.001. Thus there is a statistically significant difference in the population mean number of beetles trapped using different coloured boards, at any significance level.

```{r}
aovObject <- aov(model)
TukeyHSD(aovObject)
```

Tukey’s HSD test at a 5% significance level indicated that the population mean numbers of insects trapped by the Lemon and Green coloured traps were statistically significantly different (p = 0.004). The population mean numbers trapped by the Lemon and Green traps were statistically significantly higher than each of the White and Blue coloured traps (lemon v white p < 0.001; lemon v blue p < 0.001; green v white p = 0.003; green v blue p = 0.002).  The lemon coloured traps attracted the most number of beetles (47.2 on average) and green was the next highest (31.5 on average). The population mean numbers of insects trapped by the White and Blue traps were not statistically significantly different from each other (p = 0.996).

#### 2. Carry out the Kruskal-Wallis test. State the hypotheses, test statistic, p-value and conclusion.

Ⓡ use the function `kruskal.test()` for the Kruskal-Wallis. Use `pSDCFlig()` from the `NSM3` package for pairwise comparisons. Use `method="Asymptotic"`.

Ⓥ Analyses -> ANOVA -> Non-parametric -> One Way ANOVA (Kruskal-Wallis)

##### Solutions

Ⓢ 

Note that the ANOVA assumptions were met so not need to undertake Kruskal-Wallis; however, we have done so to become familiar with the process and to compare the two methods and their results.

$H_0:$  The population distributions of numbers of beetles trapped is the same for each of the lemon, green, blue or white coloured traps

$H_a:$  The population distribution of number of beetles trapped for at least one of these coloured traps differs from the others.

```{r}
kruskal.test(CountTrapped ~ ColourGrp, data)
```

p=0.001, so reject the null at 5% sig level (actually any level) and conclude the population distribution of number of beetles trapped is different for at least one of the lemon, green, blue or white coloured traps.  From the parallel boxplot, lemon appears to be best and blue and white the worst, but need to tease out further through formal post-hoc comparisons.

```{r}
library(NSM3)
pSDCFlig(x=data$CountTrapped, g=data$ColourGrp, method="Asymptotic")
```

The population distributions of numbers of beetles trapped by the lemon coloured traps were found to be statistically significantly different to each of the white (p=0.020) and blue (p=0.021) coloured traps, but not the green traps (p=0.051).

No other statistically significant differences between the population distributions of numbers of beetles trapped for each of the Green, White and Blue traps were found.

The lemon coloured traps attracted the most number of beetles (47.2 on average).

#### 3. Do the ANOVA and Kruskal-Wallis test lead you to the same or different conclusions? Which test is more appropriate and why?

##### Solutions

Ⓢ 

Regarding the overall test, they each rejected their respective null hypotheses, thus identifying that at least one population being compared differed; however, they each test slightly different hypotheses. For oneway fixed effects ANOVA it tests whether population means are the same or not, whereas Kruskal-Wallis tests whether the populations (not population means alone) are identical or not. 

The post-hoc multiple comparisons similarly had slightly different hypotheses, but additionally the ANOVA post-hoc tests detected more differences between coloured traps than the Kruskal-Wallis post-hoc did. The former identified population mean numbers of insects trapped by the Lemon traps as statistically significantly different to population mean numbers of insects trapped by the Green traps. 

Of the two methods in question, the parametric test (ANOVA) is a more powerful test than the non-parametric equivalent, hence whenever it is possible to use the parametric test it is preferable. 
